{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='CartPoleSwingUp', entry_point='myCartpoleF_SwingUp:CartPoleSwingUp', reward_threshold=-38.0, nondeterministic=False, max_episode_steps=24000, order_enforce=True, disable_env_checker=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=None, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import signal\n",
    "\n",
    "# 定义环境的相关设置\n",
    "balance_time = 240\n",
    "h_in = 1 / 100\n",
    "\n",
    "# 注册和创建自定义环境\n",
    "CartPoleSwingUp = gym.register(\n",
    "    id = 'CartPoleSwingUp',\n",
    "    entry_point = 'myCartpoleF_SwingUp:CartPoleSwingUp',  # 根据你的环境路径修改\n",
    "    reward_threshold = -40 * 0.95,\n",
    "    max_episode_steps = int(balance_time / h_in),\n",
    ")\n",
    "env = gym.make('CartPoleSwingUp', render_mode='human')\n",
    "print(gym.spec('CartPoleSwingUp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 NormalActionNoise 实例，用于在智能体的动作中加入噪声。具体的参数含义如下：\n",
    "\n",
    "mean = np.zeros(env.action_space.shape): 这里的 mean 是噪声的均值，np.zeros(env.action_space.shape) 表示均值为零，且它的维度与环境的动作空间 env.action_space.shape 相同。env.action_space.shape 表示动作空间的维度。\n",
    "\n",
    "sigma = 0.1 * np.ones(env.action_space.shape): 这里的 sigma 是噪声的标准差，0.1 * np.ones(env.action_space.shape) 表示标准差是 0.1，并且与动作空间的维度相同。这样每个维度的噪声标准差都是 0.1。\n",
    "\n",
    "通过这种方式，动作中会加入一定的噪声，从而使得训练过程更加稳定，防止模型陷入局部最优解，增强探索性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 DDPG 模型：\n",
    "\n",
    "'MlpPolicy': 选择了多层感知器（MLP）作为策略网络的结构。该网络用于从状态中输出动作。\n",
    "\n",
    "env: 环境对象，智能体将在该环境中进行训练。\n",
    "\n",
    "policy_kwargs = dict(net_arch=[400, 300]): 这个参数定义了策略网络的结构，net_arch=[400, 300] 表示策略网络有两层隐藏层，分别有 400 个神经元和 300 个神经元。policy_kwargs 是传递给策略网络构造函数的附加参数。\n",
    "\n",
    "verbose = 1: 这个参数设置了训练时的输出级别，verbose=1 表示打印训练过程中的信息。\n",
    "\n",
    "tensorboard_log = \"./ddpg_cartpole/\": 这个参数指定了 TensorBoard 日志的存储路径，用于后期可视化训练过程。\n",
    "\n",
    "action_noise=action_noise: 将前面创建的噪声对象 action_noise 传递给 DDPG 模型，用于在选择动作时加入噪声。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 78       |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 234      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.74    |\n",
      "|    critic_loss     | 7.82e+07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 133      |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 351      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -81.2    |\n",
      "|    critic_loss     | 1.95e+08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 250      |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 507      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.4    |\n",
      "|    critic_loss     | 1.94e+08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 406      |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 625      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 121      |\n",
      "|    critic_loss     | 2.33e+08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 524      |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 742      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 294      |\n",
      "|    critic_loss     | 2.7e+08  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 641      |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 851      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 816      |\n",
      "|    critic_loss     | 2.67e+08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 750      |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 959      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.71e+03 |\n",
      "|    critic_loss     | 3.02e+08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 858      |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 1325     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.09e+03 |\n",
      "|    critic_loss     | 1.84e+08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1224     |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 48        |\n",
      "|    time_elapsed    | 69        |\n",
      "|    total_timesteps | 3395      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.23e+03 |\n",
      "|    critic_loss     | 1.17e+08  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3294      |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 48        |\n",
      "|    time_elapsed    | 78        |\n",
      "|    total_timesteps | 3845      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.96e+03 |\n",
      "|    critic_loss     | 2.54e+08  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3744      |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 927       |\n",
      "|    total_timesteps | 47074     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.36e+04 |\n",
      "|    critic_loss     | 1.08e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 46973     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1316      |\n",
      "|    total_timesteps | 66587     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.57e+04 |\n",
      "|    critic_loss     | 1.53e+07  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 66486     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1393      |\n",
      "|    total_timesteps | 70385     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.62e+04 |\n",
      "|    critic_loss     | 2.27e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 70284     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1404      |\n",
      "|    total_timesteps | 70906     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.61e+04 |\n",
      "|    critic_loss     | 1.7e+05   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 70805     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1413      |\n",
      "|    total_timesteps | 71379     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.57e+04 |\n",
      "|    critic_loss     | 6.7e+07   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 71278     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1426      |\n",
      "|    total_timesteps | 71995     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.71e+04 |\n",
      "|    critic_loss     | 1.55e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 71894     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1435      |\n",
      "|    total_timesteps | 72456     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.69e+04 |\n",
      "|    critic_loss     | 3.9e+05   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 72355     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1445      |\n",
      "|    total_timesteps | 72940     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.71e+04 |\n",
      "|    critic_loss     | 1.12e+08  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 72839     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1452      |\n",
      "|    total_timesteps | 73316     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.78e+04 |\n",
      "|    critic_loss     | 4.54e+07  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 73215     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 80        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1458      |\n",
      "|    total_timesteps | 73592     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.76e+04 |\n",
      "|    critic_loss     | 4.54e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 73491     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1465      |\n",
      "|    total_timesteps | 73939     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.77e+04 |\n",
      "|    critic_loss     | 4.69e+07  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 73838     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1470      |\n",
      "|    total_timesteps | 74204     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.89e+04 |\n",
      "|    critic_loss     | 5.12e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 74103     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 92        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1476      |\n",
      "|    total_timesteps | 74468     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.88e+04 |\n",
      "|    critic_loss     | 1.91e+07  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 74367     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 96        |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1485      |\n",
      "|    total_timesteps | 74917     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.93e+04 |\n",
      "|    critic_loss     | 2.68e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 74816     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 100       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1495      |\n",
      "|    total_timesteps | 75388     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.12e+04 |\n",
      "|    critic_loss     | 1.86e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 75287     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 104       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1508      |\n",
      "|    total_timesteps | 76028     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.17e+04 |\n",
      "|    critic_loss     | 2.14e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 75927     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 108       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1521      |\n",
      "|    total_timesteps | 76690     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.44e+04 |\n",
      "|    critic_loss     | 6.61e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 76589     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 112       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1537      |\n",
      "|    total_timesteps | 77461     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.42e+04 |\n",
      "|    critic_loss     | 4.74e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 77360     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 116       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1550      |\n",
      "|    total_timesteps | 78123     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.65e+04 |\n",
      "|    critic_loss     | 5.21e+07  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 78022     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 120       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1566      |\n",
      "|    total_timesteps | 78902     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.77e+04 |\n",
      "|    critic_loss     | 8.55e+07  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 78801     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 124       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1588      |\n",
      "|    total_timesteps | 79989     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.01e+04 |\n",
      "|    critic_loss     | 1.4e+06   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 79888     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 128       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1632      |\n",
      "|    total_timesteps | 82178     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.25e+04 |\n",
      "|    critic_loss     | 8.88e+07  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 82077     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 132       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 1695      |\n",
      "|    total_timesteps | 85291     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.81e+04 |\n",
      "|    critic_loss     | 5.4e+05   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 85190     |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 136       |\n",
      "|    fps             | 47        |\n",
      "|    time_elapsed    | 2131      |\n",
      "|    total_timesteps | 102162    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.24e+04 |\n",
      "|    critic_loss     | 9.63e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 102061    |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 140       |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 2479      |\n",
      "|    total_timesteps | 114968    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.43e+04 |\n",
      "|    critic_loss     | 5.46e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 114867    |\n",
      "----------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 2585     |\n",
      "|    total_timesteps | 119535   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.3e+04 |\n",
      "|    critic_loss     | 7.76e+07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 119434   |\n",
      "---------------------------------\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 148       |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 3348      |\n",
      "|    total_timesteps | 169474    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.03e+04 |\n",
      "|    critic_loss     | 1.21e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 169373    |\n",
      "----------------------------------\n",
      "now Reset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m action_noise \u001b[38;5;241m=\u001b[39m NormalActionNoise(mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape), sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mones(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m DDPG(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, policy_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m]), action_noise \u001b[38;5;241m=\u001b[39m action_noise, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddpg_cartpole_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 评估训练后的模型\u001b[39;00m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:200\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    199\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n\u001b[0;32m    203\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 步骤三：使用 DDPG 算法训练代理\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 将环境包装为向量环境\n",
    "env = DummyVecEnv([lambda: gym.make('CartPoleSwingUp', render_mode='human')])\n",
    "\n",
    "# 初始化 DDPG 代理\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "# 加入噪声\n",
    "action_noise = NormalActionNoise(mean = np.zeros(env.action_space.shape), sigma = 0.2 * np.ones(env.action_space.shape))\n",
    "model = DDPG('MlpPolicy', env, policy_kwargs = dict(net_arch=[256, 128]), action_noise = action_noise, verbose = 1)\n",
    "\n",
    "model.learn(total_timesteps = 10000000)\n",
    "\n",
    "model.save(\"ddpg_cartpole_model\")\n",
    "# 评估训练后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "now Reset\n",
      "Mean reward: 272351.59526519774, Std reward: 56422.553617599915\n",
      "now Reset\n",
      "now Reset\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = DDPG.load(\"ddpg_cartpole_model\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes = 10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# 测试模型\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()  # 可视化环境\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
