{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='CartPoleSwingUp', entry_point='myCartpoleF_SwingUp:CartPoleSwingUp', reward_threshold=-38.0, nondeterministic=False, max_episode_steps=24000, order_enforce=True, disable_env_checker=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=None, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import signal\n",
    "\n",
    "# 定义环境的相关设置\n",
    "balance_time = 240\n",
    "h_in = 1 / 100\n",
    "\n",
    "# 注册和创建自定义环境\n",
    "CartPoleSwingUp = gym.register(\n",
    "    id = 'CartPoleSwingUp',\n",
    "    entry_point = 'myCartpoleF_SwingUp:CartPoleSwingUp',  # 根据你的环境路径修改\n",
    "    reward_threshold = -40 * 0.95,\n",
    "    max_episode_steps = int(balance_time / h_in),\n",
    ")\n",
    "env = gym.make('CartPoleSwingUp', render_mode='human')\n",
    "print(gym.spec('CartPoleSwingUp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 NormalActionNoise 实例，用于在智能体的动作中加入噪声。具体的参数含义如下：\n",
    "\n",
    "mean = np.zeros(env.action_space.shape): 这里的 mean 是噪声的均值，np.zeros(env.action_space.shape) 表示均值为零，且它的维度与环境的动作空间 env.action_space.shape 相同。env.action_space.shape 表示动作空间的维度。\n",
    "\n",
    "sigma = 0.1 * np.ones(env.action_space.shape): 这里的 sigma 是噪声的标准差，0.1 * np.ones(env.action_space.shape) 表示标准差是 0.1，并且与动作空间的维度相同。这样每个维度的噪声标准差都是 0.1。\n",
    "\n",
    "通过这种方式，动作中会加入一定的噪声，从而使得训练过程更加稳定，防止模型陷入局部最优解，增强探索性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 DDPG 模型：\n",
    "\n",
    "'MlpPolicy': 选择了多层感知器（MLP）作为策略网络的结构。该网络用于从状态中输出动作。\n",
    "\n",
    "env: 环境对象，智能体将在该环境中进行训练。\n",
    "\n",
    "policy_kwargs = dict(net_arch=[400, 300]): 这个参数定义了策略网络的结构，net_arch=[400, 300] 表示策略网络有两层隐藏层，分别有 400 个神经元和 300 个神经元。policy_kwargs 是传递给策略网络构造函数的附加参数。\n",
    "\n",
    "verbose = 1: 这个参数设置了训练时的输出级别，verbose=1 表示打印训练过程中的信息。\n",
    "\n",
    "tensorboard_log = \"./ddpg_cartpole/\": 这个参数指定了 TensorBoard 日志的存储路径，用于后期可视化训练过程。\n",
    "\n",
    "action_noise=action_noise: 将前面创建的噪声对象 action_noise 传递给 DDPG 模型，用于在选择动作时加入噪声。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 616      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.1     |\n",
      "|    critic_loss     | 1.3e+06  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 515      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1106     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.8     |\n",
      "|    critic_loss     | 2.37e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1005     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 1360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -50.1    |\n",
      "|    critic_loss     | 1.76e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1259     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 2305     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 2.36e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2204     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 52       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 2583     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -223     |\n",
      "|    critic_loss     | 1.65e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2482     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 52       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 2964     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -251     |\n",
      "|    critic_loss     | 3.13e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2863     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 243       |\n",
      "|    total_timesteps | 12089     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.18e+03 |\n",
      "|    critic_loss     | 9.88e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11988     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 281       |\n",
      "|    total_timesteps | 13967     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+04 |\n",
      "|    critic_loss     | 1.72e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 13866     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 298       |\n",
      "|    total_timesteps | 14856     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.35e+04 |\n",
      "|    critic_loss     | 7.73e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 14755     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 330       |\n",
      "|    total_timesteps | 16270     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.68e+04 |\n",
      "|    critic_loss     | 1.08e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 16169     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 366       |\n",
      "|    total_timesteps | 17988     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.04e+04 |\n",
      "|    critic_loss     | 1.1e+06   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 17887     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 388       |\n",
      "|    total_timesteps | 19083     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.29e+04 |\n",
      "|    critic_loss     | 1.72e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 18982     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m action_noise \u001b[38;5;241m=\u001b[39m NormalActionNoise(mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape), sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mones(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m DDPG(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, policy_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m]), action_noise \u001b[38;5;241m=\u001b[39m action_noise, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddpg_cartpole_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 评估训练后的模型\u001b[39;00m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:203\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    202\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n\u001b[1;32m--> 203\u001b[0m \u001b[43mpolyak_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Copy running stats, see GH issue #996\u001b[39;00m\n\u001b[0;32m    205\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_batch_norm_stats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_batch_norm_stats_target, \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32md:\\Study\\anaconda\\envs\\new\\lib\\site-packages\\stable_baselines3\\common\\utils.py:472\u001b[0m, in \u001b[0;36mpolyak_update\u001b[1;34m(params, target_params, tau)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# zip does not raise an exception if length of parameters does not match.\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m zip_strict(params, target_params):\n\u001b[1;32m--> 472\u001b[0m         \u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m         th\u001b[38;5;241m.\u001b[39madd(target_param\u001b[38;5;241m.\u001b[39mdata, param\u001b[38;5;241m.\u001b[39mdata, alpha\u001b[38;5;241m=\u001b[39mtau, out\u001b[38;5;241m=\u001b[39mtarget_param\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 步骤三：使用 DDPG 算法训练代理\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 将环境包装为向量环境\n",
    "env = DummyVecEnv([lambda: gym.make('CartPoleSwingUp', render_mode='human')])\n",
    "\n",
    "# 初始化 DDPG 代理\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "# 加入噪声\n",
    "action_noise = NormalActionNoise(mean = np.zeros(env.action_space.shape), sigma = 0.2 * np.ones(env.action_space.shape))\n",
    "model = DDPG('MlpPolicy', env, policy_kwargs = dict(net_arch=[256, 128]), action_noise = action_noise, verbose = 1)\n",
    "\n",
    "model.learn(total_timesteps = 100000)\n",
    "\n",
    "model.save(\"ddpg_cartpole_model\")\n",
    "# 评估训练后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = DDPG.load(\"ddpg_cartpole_model\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes = 10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# 测试模型\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()  # 可视化环境\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
