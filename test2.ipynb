{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import signal\n",
    "\n",
    "# 定义环境的相关设置\n",
    "balance_time = 240\n",
    "h_in = 1 / 100\n",
    "\n",
    "# 注册和创建自定义环境\n",
    "CartPoleSwingUp = gym.register(\n",
    "    id = 'CartPoleSwingUp',\n",
    "    entry_point = 'myCartpoleF_SwingUp:CartPoleSwingUp', \n",
    "    # reward_threshold = -40 * 0.95,\n",
    "    max_episode_steps = 10000,                                                  #int(balance_time / h_in)\n",
    ")\n",
    "env = gym.make('CartPoleSwingUp', render_mode='human')\n",
    "print(gym.spec('CartPoleSwingUp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 NormalActionNoise 实例，用于在智能体的动作中加入噪声。具体的参数含义如下：\n",
    "\n",
    "mean = np.zeros(env.action_space.shape): 这里的 mean 是噪声的均值，np.zeros(env.action_space.shape) 表示均值为零，且它的维度与环境的动作空间 env.action_space.shape 相同。env.action_space.shape 表示动作空间的维度。\n",
    "\n",
    "sigma = 0.1 * np.ones(env.action_space.shape): 这里的 sigma 是噪声的标准差，0.1 * np.ones(env.action_space.shape) 表示标准差是 0.1，并且与动作空间的维度相同。这样每个维度的噪声标准差都是 0.1。\n",
    "\n",
    "通过这种方式，动作中会加入一定的噪声，从而使得训练过程更加稳定，防止模型陷入局部最优解，增强探索性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 DDPG 模型：\n",
    "\n",
    "'MlpPolicy': 选择了多层感知器（MLP）作为策略网络的结构。该网络用于从状态中输出动作。\n",
    "\n",
    "env: 环境对象，智能体将在该环境中进行训练。\n",
    "\n",
    "policy_kwargs = dict(net_arch=[400, 300]): 这个参数定义了策略网络的结构，net_arch=[400, 300] 表示策略网络有两层隐藏层，分别有 400 个神经元和 300 个神经元。policy_kwargs 是传递给策略网络构造函数的附加参数。\n",
    "\n",
    "verbose = 1: 这个参数设置了训练时的输出级别，verbose=1 表示打印训练过程中的信息。\n",
    "\n",
    "tensorboard_log = \"./ddpg_cartpole/\": 这个参数指定了 TensorBoard 日志的存储路径，用于后期可视化训练过程。\n",
    "\n",
    "action_noise=action_noise: 将前面创建的噪声对象 action_noise 传递给 DDPG 模型，用于在选择动作时加入噪声。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RewardHistoryCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardHistoryCallback, self).__init__(verbose)\n",
    "        self.reward_list = []  # 用于存储每个 episode 的累计奖励\n",
    "\n",
    "    def _on_step(self):\n",
    "        # 获取当前 episode 的奖励\n",
    "        if self.n_calls % self.model.n_envs == 0:  # 每个 episode 结束时\n",
    "            self.reward_list.append(self.locals['rewards'].mean())\n",
    "        return True\n",
    "\n",
    "    def get_rewards(self):\n",
    "        return self.reward_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤三：使用 DDPG 算法训练代理\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "# 将环境包装为向量环境\n",
    "# env = DummyVecEnv([lambda: gym.make('CartPoleSwingUp', render_mode='human')])\n",
    "\n",
    "# 初始化 DDPG 代理\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "# 加入噪声\n",
    "\n",
    "model = DDPG('MlpPolicy', env, policy_kwargs = dict(net_arch=[256, 128, 64]), verbose = 1)\n",
    "\n",
    "reward_callback = RewardHistoryCallback()\n",
    "model.learn(total_timesteps = 100000, callback=reward_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 获取奖励历史\n",
    "reward_history = reward_callback.get_rewards()\n",
    "\n",
    "# 平滑奖励（可选）\n",
    "def smooth_rewards(rewards, window_size=10):\n",
    "    return np.convolve(rewards, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "smoothed_rewards = smooth_rewards(reward_history, window_size=10)\n",
    "\n",
    "# 绘制奖励曲线\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Smoothed Reward')\n",
    "plt.title('DDPG Training Reward Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ddpg_cartpole_model3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# 将环境包装为向量环境\n",
    "# env = DummyVecEnv([lambda: gym.make('CartPoleSwingUp', render_mode='human')])\n",
    "\n",
    "# 初始化 PPO 代理\n",
    "model = PPO('MlpPolicy', env, policy_kwargs=dict(net_arch=[256, 128, 64]), verbose = 1)\n",
    "\n",
    "# 训练模型\n",
    "model.learn(total_timesteps = 500000)\n",
    "\n",
    "# 保存训练后的模型\n",
    "model.save(\"ppo_cartpole_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# 将环境包装为向量环境\n",
    "\n",
    "# 初始化 SAC 代理\n",
    "model = SAC('MlpPolicy', env, policy_kwargs=dict(net_arch=[256, 128, 64]), verbose=1, learning_rate=0.001)\n",
    "\n",
    "# 训练模型\n",
    "model.learn(total_timesteps=30000)\n",
    "\n",
    "# 保存训练后的模型\n",
    "model.save(\"sac_cartpole_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.td3.policies import TD3Policy\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# 初始化 TD3 代理\n",
    "model = TD3('MlpPolicy', env, policy_kwargs=dict(net_arch=[256, 128, 64]), verbose = 1)\n",
    "\n",
    "# 训练模型\n",
    "model.learn(total_timesteps = 200000)\n",
    "\n",
    "# 保存训练后的模型\n",
    "model.save(\"td3_cartpole_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "model = DDPG.load(\"ddpg_cartpole_model3\", env = env)\n",
    "reward_callback = RewardHistoryCallback()\n",
    "model.learn(total_timesteps = 100000, callback=reward_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 获取奖励历史\n",
    "reward_history = reward_callback.get_rewards()\n",
    "\n",
    "# 平滑奖励（可选）\n",
    "def smooth_rewards(rewards, window_size=10):\n",
    "    return np.convolve(rewards, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "smoothed_rewards = smooth_rewards(reward_history, window_size=10)\n",
    "\n",
    "# 绘制奖励曲线\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Smoothed Reward')\n",
    "plt.title('DDPG Training Reward Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ddpg_cartpole_model3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DDPG\n",
    "model = DDPG.load(\"ddpg_cartpole_model\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes = 10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DDPG\n",
    "model = DDPG.load(\"ddpg_cartpole_model2\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes = 10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DDPG\n",
    "model = DDPG.load(\"ddpg_cartpole_model3\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes = 10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselinecs3 import PPO\n",
    "model = PPO.load(\"ppo_cartpole_model\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes = 10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
